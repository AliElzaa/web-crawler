# web-crawler

With this web crawler I had these principles in mind:

1. Scalability: How can you scale the project for multiple URL's and adhere to SOLID principles and 'clean code'? The way forward was to seperate components where needed and to keep code easy to read and in functions.

2. Effeciency: in terms of time-space complexity, the project did not require a lot of traversing through data structures, so I tried to avoid loops as much as possible and tried to keep the time complexity as O(N).
